#Voice Interface for Restaurant Search
* **Motivation**

Yelp, Foursquare and other websites provide local business information that help customers find and discover restaurants they like, by getting specific information about them, and even making table reservations. A voice interface for such applications allows the hands-free usage of such applications. This can be highly efficient, as users mainly use these applications when they are on the move. Typing in filters, and searching for various details through the small screen of their phones can become quite tedious, and cumbersome.

Our goal is to build a spoken dialogue system that lets users search for restaurants. This system recommends restaurants based on preferences specified by users, and provides information such as address, hours of operation, popularity, price range and contact information.

There are several challenges we are facing. A primary challenge is detection of a new query, and subsequently resetting filters to the request. One possible solution is to develop keywords. For example, we could ask users to say “Hooray!” or “Thanks <Application Name>” to represent end of conversation, or “Start over” to reset filters and begin a new query. Another challenge is to efficiently present the information in order to enhance user experience. Given the large number of restaurants listed in each result, and the amount of information associated with them, it is essential we optimize our information presentation algorithms. This is essential in order to ensure that the users are not overburdened by the information, and to minimize the number of dialogue turns required for the user to find an acceptable option. We plan to use a combination of speech, text and pictures to optimize this.

* **System functionality**

The major functionality of the system is to recommend restaurants to the user, based on his/her stated preferences. The system will use speech as input and the output will consist of a combination of template-generated speech and on-screen text and pictures. There are mainly two types of actions a user could perform. The first is to get recommendations of restaurants based on their preferences. An example is shown in Figure 1. The second is to get information about specific restaurants, as in Figure 2. For a specific restaurant, users can request information, such as phone number, photos, distance, and deals after a restaurant is selected. 

The domain concepts of our system includes sorting criterion, location, rating and cuisine. Listed below are some example concepts:

   1. cuisine: Indian, Japanese, Thai
   2. sorting criterion: rating, distance, price
   3. rating: One Star, Two Stars, Three Stars, Four Stars, Five Stars
   4. location: Upper West Side, Morningside Heights, nearby, within one mile

![Sample Dialogue 1](http://i.imgur.com/voYykBz.jpg)
![Sample Dialogue 2](http://i.imgur.com/khTuWSG.jpg)

* **Implementation**

![Control Flow](http://i.imgur.com/WyOqx4A.jpg)

We will develop a web application using Flask Python web framework. The frontend of our web app will be in HTML, CSS, and JavaScript. The interface of our application will have three main components. The first is a button that the user presses to start a dialogue. The second component is a textbox that displays the transcription of the conversation between the user and the system. The third component displays the text output and graphics. Figure 3 illustrates what the web interface will look like. Figure 4 shows the control flow of our dialogue system. The wit.ai framework will be used for speech recognition and natural language understanding. We will implement a simple dialogue manager (DM) using Python. The dialogue manager will manage the dialogue state and information provided by user. Our dialogue system will interface with the Yelp API. This allows for expandability in the future. A parser, which parses JSON objects returned by wit.ai and generates requests to query information from Yelp Search API, will be implemented. The Google Location Services API may be integrated to the system later on to add location awareness to the application. More details on the DM are below. The natural language generation component will be rule-based and we will draft the templates. We plan to use MaryTTS for text-to-speech synthesis, as it is open source and is sufficient for our purposes. 

We decided on wit.ai because of its built-in speech recognition and natural language understanding capabilities. Prior to use, however, the wit.ai instance requires NLU training. At the highest level, wit.ai recognizes what are called “intents”. These are general intentions of the user utterance, as specified by the system designer. The list of intents we used can be found in Table 1 below.  Wit.ai can also tag entities, which are words or phrases that we want to extract meaning from. To train this system, we gave the system several example inputs which are similar to what the user would say to the system. We manually tag these sample inputs with an intent, and we tag any entities that may be present. Table 2 below lists some example inputs that we used, along with the corresponding intents and entities. We input these and related sample inputs until our system begins to recognize them. We train the system using primarily text input, as wit.ai has speech recognition built in. However, we do also test the system with voice input to see if the speech recognition and intent/entity tagging are correct. Wit.ai can recognize popular restaurant names, such as Starbucks and McDonald’s. We will limit the support of the initial version of our system to restaurants near Columbia University and enter all possible restaurant names to train wit.ai.

Figure 5 is the state diagram of our dialogue manager. Our system greets the user at the introduction stage. It then takes an initial request from the the user and enters the restaurant search stage, which will be implemented in a slot-filling fashion. We have four slots: cuisine (or a general search term), location/distance, sorting criterion and whether to exclude those without deals. Those are the criteria that the user can search on. The dialogue manager detects which slots have not been filled yet and prompts user with questions. The user can either provide his/her preference or say that he/she does not have any preferences. After all slots are filled, the DM calls the function that queries the Yelp Search API to get a list of restaurants and goes into the restaurant selection stage, where the system will display a numbered list of results and ask the user to select one restaurant of his/her interest. The user can select a restaurant by specifying its its number on the list. After a user selection is specified, the system enters the  restaurant information stage. The system shows some basic information of the restaurant, such as name, image and star rating. The user can ask for more specific information such as phone number, address, and whether the restaurant is open. After that the user can choose to finish dialogue or start a new search. At any point in time during the dialogue, the user can ask to go back to the beginning and start a new search.


As mentioned, the third-party API we will use is Yelp Search API and Yelp Business API. Yelp provides a search API that allows user to search for restaurants and other business based on location, cuisine, and restaurant name. The Yelp Business API provides information, such as image, location, review and phone number of a restaurant. We write a python program that takes cuisine, location, and sorting preferences and returns restaurant information of top results. The source code is in our github repository. It will be parsed by the dialogue manager at the restaurant info stage. Following is a sample Yelp API query: 

http://api.yelp.com/v2/search?term=Italian&location=Columbia+University&limit=1

Our main focus is on system development and we aim to develop an end-to-end fully functioning web application. We may also look into research questions related to information presentation. To evaluate our system, we will ask 20 users to independently try our system and fill out an evaluation survey. This will consist of 10 predetermined scenarios for each user, such as “Find the address for the nearest Chinese restaurant.” The users will be asked to perform the tasks to the best of their ability and score based on their experience. Additionally, the users will be asked to rate or discuss the usability and visual appeal of the system and report what they liked or disliked about the system.  We will also evaluate based on user completion of the tasks, both actual completion and completion from the user perspective.

* **Evaluation**

Our main focus is on system development and we aim to develop an end­to­end fully functioning mobile app. We may also look into research questions related to information presentation. To evaluate our system, we will create an evaluation questionnaire. The survey will have questions based on level of measurement as well as open­ended questions asking for feedbacks. We intend to ask users to test and interact with our system and fill out the evaluation questionnaire to judge their satisfaction with their experience.

* **Related Work**

Popular virtual assistant applications such as Siri, Google Now, and Vlingo provide restaurant information. Siri can take restaurant-related requests and queries the Yelp API to provide relative information using a combination of text, images and speech. Google Now uses Google Business Services API and gives users speech responses and Google search results. For both systems, users are able to ask for information with a single question, such as “What is the phone number of Starbucks”. However, neither of them supports multi-turn dialogues in the restaurant/business domain.  Our system will support multi-turn dialogues and the users will be able to ask for different information about the same restaurant in context without specifying its name in each of his/her request.

Computer scientists in the research community have also studied and developed dialogue systems of restaurant domain. MATCH[1] is among the first restaurant search dialogue systems that worked on mobile devices. It combines finite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output. We may not have enough time to develop a multimodal system for our project. However, our system will be more powerful in information-providing because we are using the Yelp API and our system has the potential to be expanded to include restaurants around the world. MATCH can only provide information for restaurant in NYC. MVA[2] is a state-of-the-art system developed by researchers from AT&T Research recently. It is a multimodal application that allows users to interact with it using speech and gestures. Our goal for this project is to develop a web application that is similar to MVA.

